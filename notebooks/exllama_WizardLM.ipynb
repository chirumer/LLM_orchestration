{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RImvbMIJ9P3C",
        "outputId": "a6835ced-1e35-4970-d927-ecde55df728b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'WizardLM-13B-V1.2-GPTQ' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/TheBloke/WizardLM-13B-V1.2-GPTQ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/turboderp/exllama\n",
        "cd exllama\n",
        "\n",
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQVQrRnx9a0H",
        "outputId": "a7861dd1-0313-46af-8ed5-f66c66550beb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'exllama' already exists and is not an empty directory.\n",
            "Requirement already satisfied: torch>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.1.0+cu121)\n",
            "Requirement already satisfied: safetensors==0.3.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (0.3.2)\n",
            "Requirement already satisfied: sentencepiece>=0.1.97 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.1.99)\n",
            "Requirement already satisfied: ninja==1.11.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.1->-r requirements.txt (line 1)) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.1->-r requirements.txt (line 1)) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0.1->-r requirements.txt (line 1)) (1.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd exllama"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkpGapiGA9Y5",
        "outputId": "53ee2e03-ef8f-4a92-ff09-a5f0bd4379cd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/exllama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
        "from lora import ExLlamaLora\n",
        "from tokenizer import ExLlamaTokenizer\n",
        "from generator import ExLlamaGenerator\n",
        "import argparse\n",
        "import torch\n",
        "import sys\n",
        "import os\n",
        "import glob\n",
        "import model_init\n",
        "\n",
        "# Simple interactive chatbot script\n",
        "\n",
        "torch.set_grad_enabled(False)\n",
        "torch.cuda._lazy_init()\n",
        "\n",
        "# Parse arguments\n",
        "\n",
        "parser = argparse.ArgumentParser(description = \"Simple chatbot example for ExLlama\")\n",
        "\n",
        "model_init.add_args(parser)\n",
        "\n",
        "parser.add_argument(\"-lora\", \"--lora\", type = str, help = \"Path to LoRA binary to use during benchmark\")\n",
        "parser.add_argument(\"-loracfg\", \"--lora_config\", type = str, help = \"Path to LoRA config to use during benchmark\")\n",
        "parser.add_argument(\"-ld\", \"--lora_dir\", type = str, help = \"Path to LoRA config and binary. to use during benchmark\")\n",
        "\n",
        "parser.add_argument(\"-p\", \"--prompt\", type = str, help = \"Prompt file\")\n",
        "parser.add_argument(\"-un\", \"--username\", type = str, help = \"Display name of user\", default = \"User\")\n",
        "parser.add_argument(\"-bn\", \"--botname\", type = str, help = \"Display name of chatbot\", default = \"Chatbort\")\n",
        "parser.add_argument(\"-bf\", \"--botfirst\", action = \"store_true\", help = \"Start chat on bot's turn\")\n",
        "\n",
        "parser.add_argument(\"-nnl\", \"--no_newline\", action = \"store_true\", help = \"Do not break bot's response on newline (allow multi-paragraph responses)\")\n",
        "parser.add_argument(\"-temp\", \"--temperature\", type = float, help = \"Temperature\", default = 0.95)\n",
        "parser.add_argument(\"-topk\", \"--top_k\", type = int, help = \"Top-K\", default = 20)\n",
        "parser.add_argument(\"-topp\", \"--top_p\", type = float, help = \"Top-P\", default = 0.65)\n",
        "parser.add_argument(\"-minp\", \"--min_p\", type = float, help = \"Min-P\", default = 0.00)\n",
        "parser.add_argument(\"-repp\",  \"--repetition_penalty\", type = float, help = \"Repetition penalty\", default = 1.15)\n",
        "parser.add_argument(\"-repps\", \"--repetition_penalty_sustain\", type = int, help = \"Past length for repetition penalty\", default = 256)\n",
        "parser.add_argument(\"-beams\", \"--beams\", type = int, help = \"Number of beams for beam search\", default = 1)\n",
        "parser.add_argument(\"-beamlen\", \"--beam_length\", type = int, help = \"Number of future tokens to consider\", default = 1)\n",
        "\n",
        "args = parser.parse_args('-d ../WizardLM-13B-V1.2-GPTQ -un \"Jeff\" -p prompt_chatbort.txt'.split())\n",
        "model_init.post_parse(args)\n",
        "model_init.get_model_files(args)\n",
        "\n",
        "# Paths\n",
        "\n",
        "if args.lora_dir is not None:\n",
        "    args.lora_config = os.path.join(args.lora_dir, \"adapter_config.json\")\n",
        "    args.lora = os.path.join(args.lora_dir, \"adapter_model.bin\")\n",
        "\n",
        "# Some feedback\n",
        "\n",
        "print(f\" -- Sequence length: {args.length}\")\n",
        "print(f\" -- Temperature: {args.temperature:.2f}\")\n",
        "print(f\" -- Top-K: {args.top_k}\")\n",
        "print(f\" -- Top-P: {args.top_p:.2f}\")\n",
        "print(f\" -- Min-P: {args.min_p:.2f}\")\n",
        "print(f\" -- Repetition penalty: {args.repetition_penalty:.2f}\")\n",
        "print(f\" -- Beams: {args.beams} x {args.beam_length}\")\n",
        "\n",
        "print_opts = []\n",
        "if args.no_newline: print_opts.append(\"no_newline\")\n",
        "if args.botfirst: print_opts.append(\"botfirst\")\n",
        "\n",
        "model_init.print_options(args, print_opts)\n",
        "\n",
        "# Globals\n",
        "\n",
        "model_init.set_globals(args)\n",
        "\n",
        "# Load prompt file\n",
        "\n",
        "username = args.username\n",
        "bot_name = args.botname\n",
        "\n",
        "if args.prompt is not None:\n",
        "    with open(args.prompt, \"r\") as f:\n",
        "        past = f.read()\n",
        "        past = past.replace(\"{username}\", username)\n",
        "        past = past.replace(\"{bot_name}\", bot_name)\n",
        "        past = past.strip() + \"\\n\"\n",
        "else:\n",
        "    past = f\"{bot_name}: Hello, {username}\\n\"\n",
        "\n",
        "# past += \"User: Hi. Please say \\\"Shhhhhh\\\"?\\n\"\n",
        "# args.botfirst = True\n",
        "\n",
        "# Instantiate model and generator\n",
        "\n",
        "config = model_init.make_config(args)\n",
        "\n",
        "model = ExLlama(config)\n",
        "cache = ExLlamaCache(model)\n",
        "tokenizer = ExLlamaTokenizer(args.tokenizer)\n",
        "\n",
        "model_init.print_stats(model)\n",
        "\n",
        "# Load LoRA\n",
        "\n",
        "lora = None\n",
        "if args.lora:\n",
        "    print(f\" -- LoRA config: {args.lora_config}\")\n",
        "    print(f\" -- Loading LoRA: {args.lora}\")\n",
        "    if args.lora_config is None:\n",
        "        print(f\" ## Error: please specify lora path to adapter_config.json\")\n",
        "        sys.exit()\n",
        "    lora = ExLlamaLora(model, args.lora_config, args.lora)\n",
        "    if lora.bias_ignored:\n",
        "        print(f\" !! Warning: LoRA zero bias ignored\")\n",
        "\n",
        "# Generator\n",
        "\n",
        "generator = ExLlamaGenerator(model, tokenizer, cache)\n",
        "generator.settings = ExLlamaGenerator.Settings()\n",
        "generator.settings.temperature = args.temperature\n",
        "generator.settings.top_k = args.top_k\n",
        "generator.settings.top_p = args.top_p\n",
        "generator.settings.min_p = args.min_p\n",
        "generator.settings.token_repetition_penalty_max = args.repetition_penalty\n",
        "generator.settings.token_repetition_penalty_sustain = args.repetition_penalty_sustain\n",
        "generator.settings.token_repetition_penalty_decay = generator.settings.token_repetition_penalty_sustain // 2\n",
        "generator.settings.beams = args.beams\n",
        "generator.settings.beam_length = args.beam_length\n",
        "\n",
        "generator.lora = lora\n",
        "\n",
        "break_on_newline = not args.no_newline\n",
        "\n",
        "# Be nice to Chatbort\n",
        "\n",
        "min_response_tokens = 4\n",
        "max_response_tokens = 256\n",
        "extra_prune = 256\n",
        "\n",
        "print(past, end = \"\")\n",
        "ids = tokenizer.encode(past)\n",
        "generator.gen_begin(ids)\n",
        "\n",
        "next_userprompt = username + \": \"\n",
        "\n",
        "first_round = True\n",
        "\n",
        "while True:\n",
        "\n",
        "    res_line = bot_name + \":\"\n",
        "    res_tokens = tokenizer.encode(res_line)\n",
        "    num_res_tokens = res_tokens.shape[-1]  # Decode from here\n",
        "\n",
        "    if first_round and args.botfirst: in_tokens = res_tokens\n",
        "\n",
        "    else:\n",
        "\n",
        "        # Read and format input\n",
        "\n",
        "        in_line = input(next_userprompt)\n",
        "        in_line = username + \": \" + in_line.strip() + \"\\n\"\n",
        "\n",
        "        next_userprompt = username + \": \"\n",
        "\n",
        "        # No need for this, really, unless we were logging the chat. The actual history we work on is kept in the\n",
        "        # tokenized sequence in the generator and the state in the cache.\n",
        "\n",
        "        past += in_line\n",
        "\n",
        "        # SentencePiece doesn't tokenize spaces separately so we can't know from individual tokens if they start a new word\n",
        "        # or not. Instead, repeatedly decode the generated response as it's being built, starting from the last newline,\n",
        "        # and print out the differences between consecutive decodings to stream out the response.\n",
        "\n",
        "        in_tokens = tokenizer.encode(in_line)\n",
        "        in_tokens = torch.cat((in_tokens, res_tokens), dim = 1)\n",
        "\n",
        "    # If we're approaching the context limit, prune some whole lines from the start of the context. Also prune a\n",
        "    # little extra so we don't end up rebuilding the cache on every line when up against the limit.\n",
        "\n",
        "    expect_tokens = in_tokens.shape[-1] + max_response_tokens\n",
        "    max_tokens = config.max_seq_len - expect_tokens\n",
        "    if generator.gen_num_tokens() >= max_tokens:\n",
        "        generator.gen_prune_to(config.max_seq_len - expect_tokens - extra_prune, tokenizer.newline_token_id)\n",
        "\n",
        "    # Feed in the user input and \"{bot_name}:\", tokenized\n",
        "\n",
        "    generator.gen_feed_tokens(in_tokens)\n",
        "\n",
        "    # Generate with streaming\n",
        "\n",
        "    print(res_line, end = \"\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    generator.begin_beam_search()\n",
        "\n",
        "    for i in range(max_response_tokens):\n",
        "\n",
        "        # Disallowing the end condition tokens seems like a clean way to force longer replies.\n",
        "\n",
        "        if i < min_response_tokens:\n",
        "            generator.disallow_tokens([tokenizer.newline_token_id, tokenizer.eos_token_id])\n",
        "        else:\n",
        "            generator.disallow_tokens(None)\n",
        "\n",
        "        # Get a token\n",
        "\n",
        "        gen_token = generator.beam_search()\n",
        "\n",
        "        # If token is EOS, replace it with newline before continuing\n",
        "\n",
        "        if gen_token.item() == tokenizer.eos_token_id:\n",
        "            generator.replace_last_token(tokenizer.newline_token_id)\n",
        "\n",
        "        # Decode the current line and print any characters added\n",
        "\n",
        "        num_res_tokens += 1\n",
        "        text = tokenizer.decode(generator.sequence_actual[:, -num_res_tokens:][0])\n",
        "        new_text = text[len(res_line):]\n",
        "\n",
        "        skip_space = res_line.endswith(\"\\n\") and new_text.startswith(\" \")  # Bit prettier console output\n",
        "        res_line += new_text\n",
        "        if skip_space: new_text = new_text[1:]\n",
        "\n",
        "        print(new_text, end=\"\")  # (character streaming output is here)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        # End conditions\n",
        "\n",
        "        if break_on_newline and gen_token.item() == tokenizer.newline_token_id: break\n",
        "        if gen_token.item() == tokenizer.eos_token_id: break\n",
        "\n",
        "        # Some models will not (or will inconsistently) emit EOS tokens but in a chat sequence will often begin\n",
        "        # generating for the user instead. Try to catch this and roll back a few tokens to begin the user round.\n",
        "\n",
        "        if res_line.endswith(f\"{username}:\"):\n",
        "            plen = tokenizer.encode(f\"{username}:\").shape[-1]\n",
        "            generator.gen_rewind(plen)\n",
        "            next_userprompt = \" \"\n",
        "            break\n",
        "\n",
        "    generator.end_beam_search()\n",
        "\n",
        "    past += res_line\n",
        "    first_round = False"
      ],
      "metadata": {
        "id": "SxAzh0c7CoB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xmBF6rWLFTXA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}